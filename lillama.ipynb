{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNATXC3VGaxOaeyONgltwEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/lillama/blob/master/lillama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lillama: Large Language Models Compression via Low-Rank Feature Distillation\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/user-attachments/assets/31ea5289-ca53-4d5e-a853-2ba8f357c24f?raw=true:, width=500\" alt=\"lillama\" width=500 class=\"center\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "RxM20A9c0Npv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for to reproduce the results for Phi-2 3B. Note that this can run on a single GPU T4 (Colab free tier), but it will be slow (>20hrs). So I suggest you to use the A100-40GB GPU."
      ],
      "metadata": {
        "id": "x7yHml4E100M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "If you're working with GPUs of capability < 8.0, you can ignore the flash-attn installation."
      ],
      "metadata": {
        "id": "bXcddXT40m_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiNaAQZpoRxh"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/yaya-sy/lillama.git flash-attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compression\n",
        "Here is an example of how you can compress Phi-2 3B by 20%.\n",
        "\n",
        "## Step 1: Prepare the dataset"
      ],
      "metadata": {
        "id": "GXvek0xo0qWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare dataset of 13 million tokens from Slim-Orca (sharegpt format). You can also use the Alpaca format.\n",
        "!HF_DATASETS_TRUST_REMOTE_CODE=True lillama-sharegpt \\\n",
        "  --tokenizer microsoft/phi-2 \\\n",
        "  --dataset Open-Orca/SlimOrca \\\n",
        "  --subset 13_000_000 \\\n",
        "  --output-folder distillation-data"
      ],
      "metadata": {
        "id": "ESM_Ot2OoaXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This will prepare Slim-Orca (sharegpt format) for distillation\n",
        "\n",
        "## Step 2: Compress the model"
      ],
      "metadata": {
        "id": "rGNMU_IX0x98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# will compress phi-2 3B by 20%\n",
        "!HF_DATASETS_TRUST_REMOTE_CODE=True lillama-distill \\\n",
        "  --llm microsoft/phi-2 \\\n",
        "  --train-data distillation-data/ \\\n",
        "  --output-folder distilled-phi2/ \\\n",
        "  --reduction 20 \\\n",
        "  --batch-size 8 \\\n",
        "  --log-interval 256"
      ],
      "metadata": {
        "id": "YDwu5IR5pr8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For big models (for example Mixtral-47B), you should use the argument `--no-evaluate` so the whole model will not be loaded on GPU.\n",
        "\n",
        "The distilled weights will be saved in `distilled-phi2/checkpoints`."
      ],
      "metadata": {
        "id": "CusgtYy806x0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "## 0-shot evaluation with `lm-eval'\n",
        "You can evaluate the compressed model as:"
      ],
      "metadata": {
        "id": "xTMq_Lcr1BC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_DATASETS_TRUST_REMOTE_CODE=True python -m lillama.evaluation.lm_eval \\\n",
        "  --llm microsoft/phi-2 \\\n",
        "  --distill-path distilled-phi2/checkpoints/ \\\n",
        "  --output-folder distilled-phi2-eval/"
      ],
      "metadata": {
        "id": "rqHo2g7J6FoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will save two `.json` files. The file `full_results_0_shot.json` contains the detailed results while  `0_shot_task_results.json`contains the summarized evaluation results.\n",
        "\n",
        "## Generate with the compressed model\n",
        "\n",
        "You can also manually inspect the generations of the model using Huggingface Transformers:"
      ],
      "metadata": {
        "id": "bjlbBbZC1FEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lillama.utils import load_lr_llm\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "MJY3Ma12rhDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, lr_llm, _ = load_lr_llm(checkpoint=\"microsoft/phi-2\",\n",
        "                           distill_path=\"distilled-phi2/checkpoints\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "print(f\"Number of parameters of the low-rank LLM: {lr_llm.num_parameters():,}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "M36tOKSHB-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pipe = pipeline(\"text-generation\", model=lr_llm, do_sample=True, tokenizer=tokenizer, temperature=0.3, device=device)"
      ],
      "metadata": {
        "id": "bxR5jIFeFT2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = lr_pipe(\"What is the cause of the Civil War? Here is the story:\",\n",
        "                 max_new_tokens=256,\n",
        "                 min_new_tokens=32,\n",
        "                 top_p=0.9,\n",
        "                 top_k=10)[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "F-q_JqISFyYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "5S_yFlLvaoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Share the compressed with the community on Hugginface\n",
        "\n",
        "At the moment the model can only be loaded with `lillama`. To share the model and use it independently, you should modify manually the model file.\n",
        "\n",
        "First, save the compressed model as Huggingface Model:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLLIAFwX1VSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lillama.utils import save_lr_llm"
      ],
      "metadata": {
        "id": "TKxK2uhAzFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_lr_llm(checkpoint=\"microsoft/phi-2\", distill_path=\"distilled-phi2/checkpoints\", output_folder=\"hf_phi2-2B\")"
      ],
      "metadata": {
        "id": "3hkeWBZ6zGxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This will save compressed model and its config.\n",
        "\n",
        "Then you have to modify the model file by replacing the linear layers `torch.nn.Linear(input_feature, output_features)` with the low rank ones: `torch.nn.Sequential(torch.nn.Linear(input_feature, rank), torch.nn.Linear(rank, output_features))`. I haven't automatized this, but I've done it for `Mixtral', so you can it as template: https://huggingface.co/yaya-sy/minixtral/blob/main/modeling_mixtral.py\n",
        "\n",
        "You also have to modify he config.json for Transformers `auto_map`. Please se how I achieved this here: https://huggingface.co/yaya-sy/minixtral/blob/main/config.json\n"
      ],
      "metadata": {
        "id": "Fe09qP1F1hgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use this compressed model as a normal huggingface llm. You can par example finetune it on your own data.\n",
        "\n",
        "If you want to push it on huggingface so anyone can use it, you have to create a custom model file. Please see"
      ],
      "metadata": {
        "id": "24gyUJv7ZDio"
      }
    }
  ]
}